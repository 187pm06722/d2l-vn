# Deep Learning Basics 

This chapter serves as a basic introduction to deep learning.  Deep learning, as part of a broader family of machine learning methods, is often based on a neural network model in order to represent increasingly abstract concepts or patterns on a step-by-step basis. In order to briefly introduce basic machine learning concepts, we will begin with two single-layer neural networks, linear regression and softmax regression,  then continue from single-layer neural networks to multi-layer neural networks, and finally, introduce a deep learning model through a multi-layer perceptron. After observing and understanding the model overfitting, we will introduce two frequently-adopted methods of dealing with it in deep learning: weight decay and dropout. Next, to further understand deep learning model training, we will explain forward propagation and back propagation in depth.  After a reader is familiar with these two concepts, they will better understand some of the problems with numerical stability and initialization in deep learning. Finally, we will apply the knowledge learned in this chapter to a deep learning application case.

```eval_rst

.. toctree::
   :maxdepth: 2

   linear-regression
   linear-regression-scratch
   linear-regression-gluon
   softmax-regression
   fashion-mnist
   softmax-regression-scratch
   softmax-regression-gluon
   mlp
   mlp-scratch
   mlp-gluon
   underfit-overfit
   weight-decay
   dropout
   backprop
   numerical-stability-and-init
   kaggle-house-price

```
